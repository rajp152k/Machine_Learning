# A DISCIPLINED APPROACH TO NEURAL NETWORK HYPER-PARAMETERS: PART 1 â€“ LEARNING RATE, BATCH SIZE, MOMENTUM, AND WEIGHT DECAY
	- learning rate
	- batch size
	- momentum
	- weight decay

coverage:
	- cyclical learning rates coupled with cyclical momentum for faster learning
		- LR Range test: fit_one_cycle policy: increasing and annealing combined
		- detecting over and under fits. 
	- regularizing effect of batch sizes
	- using weight decay as the final hyperparameter tuner to balance out variation in others in cycles to achieve ideal regularization with adequate training speed.

