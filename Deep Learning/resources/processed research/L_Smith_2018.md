#A DISCIPLINED APPROACH TO NEURAL NETWORK HYPER-PARAMETERS: PART 1 â€“ LEARNING RATE, BATCH SIZE, MOMENTUM, AND WEIGHT DECAY<br>
	- learning rate<br>
	- batch size<br>
	- momentum<br>
	- weight decay<br>


coverage:<br>
	- cyclical learning rates coupled with cyclical momentum for faster learning<br>
		- LR Range test: fit_one_cycle policy: increasing and annealing combined<br>
		- detecting over and under fits. <br>
	- regularizing effect of batch sizes<br>
	- using weight decay as the final hyperparameter tuner to balance out variation in others in cycles to achieve ideal regularization with adequate training speed.<br>

