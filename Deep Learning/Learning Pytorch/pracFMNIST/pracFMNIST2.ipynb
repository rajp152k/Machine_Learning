{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pracFMNIST2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KO4B2FSf3GZl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wg3TeFO-3jHn",
        "colab_type": "code",
        "outputId": "b4cd2965-9232-465e-e59b-8620a612c8e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "print(torch.__version__)\n",
        "print(torchvision.__version__)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.3.0+cu100\n",
            "0.4.1+cu100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DV4tu36W3tme",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_num_correct(preds,labels):\n",
        "    return preds.argmax(dim=1).eq(labels).sum().item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Wo9CS7u37rK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1,6,5)\n",
        "        self.conv2 = nn.Conv2d(6,12,5)\n",
        "\n",
        "        self.fc1 = nn.Linear(12*4*4,120)\n",
        "        self.fc2 = nn.Linear(120,60)\n",
        "        self.out = nn.Linear(60,10)\n",
        "\n",
        "    def forward(self,t):\n",
        "        t = F.relu(self.conv1(t))\n",
        "        t = F.max_pool2d(t,kernel_size=2,stride=2)\n",
        "        \n",
        "        t = F.relu(self.conv2(t))\n",
        "        t = F.max_pool2d(t,kernel_size=2,stride=2)\n",
        "\n",
        "        t = t.flatten(start_dim=1)\n",
        "        t = F.relu(self.fc1(t))\n",
        "\n",
        "        t = F.relu(self.fc2(t))\n",
        "\n",
        "        t = self.out(t)\n",
        "\n",
        "        return t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tX1Dd2El4g00",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_set = torchvision.datasets.FashionMNIST(\n",
        "    root = \"./data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transforms.Compose(\n",
        "        [transforms.ToTensor()]\n",
        "    )\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGhHqg0O5o8J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_set,batch_size=100,shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDoAiqu3523y",
        "colab_type": "text"
      },
      "source": [
        "Using tensorboard for network graph and images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcs-oeYbF3rp",
        "colab_type": "text"
      },
      "source": [
        "NOTE: storing the SummaryWriter outputs files on my local system to use a local host.<br>\n",
        "saved on github as well.<br>\n",
        "ngrok not working<br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ju0FlKt5NFuY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use !zip -r <zipped file> <directory being zipped>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oRpnIySNSGn",
        "colab_type": "text"
      },
      "source": [
        "The training loop review"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-K_q6jtYNqFu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "outputId": "6c7939de-b99a-4565-db0c-c3dea5ffdbbd"
      },
      "source": [
        "network = Network()\n",
        "train_loader = torch.utils.data.DataLoader(train_set,batch_size=100,shuffle=True)\n",
        "optimizer = optim.Adam(network.parameters(),lr=0.01)\n",
        "\n",
        "for epoch in range(1):\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        images,labels = batch\n",
        "        \n",
        "        preds = network(images)\n",
        "        loss = F.cross_entropy(preds,labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_correct += get_num_correct(preds,labels)\n",
        "\n",
        "    print(\"epoch\", epoch,'\\n', \"total_correct:\",total_correct,'\\n',\"loss:\",total_loss)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-9e161bf630ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \"\"\"\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aj6G1tWAPWZv",
        "colab_type": "text"
      },
      "source": [
        "now including tensorboard calls in between to get data for analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyeRFl4yQKQy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -r runs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGD9l4WRPnST",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "outputId": "258c3868-e1ca-4d46-cce7-9bd3e36439da"
      },
      "source": [
        "network = Network()\n",
        "train_loader = torch.utils.data.DataLoader(train_set,batch_size=100,shuffle=True)\n",
        "optimizer = optim.Adam(network.parameters(),lr=0.01)\n",
        "\n",
        "images,labels = next(iter(train_loader))\n",
        "grid = torchvision.utils.make_grid(images)\n",
        "\n",
        "tb = SummaryWriter(comment=f'sample run')\n",
        "tb.add_image('images',grid)\n",
        "tb.add_graph(network,images)\n",
        "\n",
        "for epoch in range(1):\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "\n",
        "    for batch in train_loader: # getting batch\n",
        "        images,labels = batch # separating images and labels\n",
        "        \n",
        "        preds = network(images) \n",
        "        loss = F.cross_entropy(preds,labels) #calculating loss\n",
        "\n",
        "        optimizer.zero_grad() \n",
        "        loss.backward()  # calculating gradients\n",
        "        optimizer.step() # updating weights\n",
        "\n",
        "        tb.add_scalar('Loss',loss,epoch)\n",
        "        tb.add_scalar('Number correct',total_correct,epoch)\n",
        "        tb.add_scalar('Accuracy',total_correct/len(train_set),epoch)\n",
        "\n",
        "        tb.add_histogram('conv1.bias',network.conv1.bias,epoch)\n",
        "        tb.add_histogram('conv1.weight',network.conv1.weight,epoch)\n",
        "        tb.add_histogram('conv1.weight.grad',network.conv1.weight.grad,epoch)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_correct += get_num_correct(preds,labels)\n",
        "\n",
        "    print(\"epoch\", epoch,'\\n', \"total_correct:\",total_correct,'\\n',\"loss:\",total_loss)\n",
        "\n",
        "tb.close()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0 \n",
            " total_correct: 47774 \n",
            " loss: 324.24461951851845\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZvWEnDXRmkQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "outputId": "9d0c573a-4a19-4595-97b4-424304479088"
      },
      "source": [
        ""
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: runs/ (stored 0%)\n",
            "  adding: runs/Oct28_16-50-13_9f396b488efb/ (stored 0%)\n",
            "  adding: runs/Oct28_16-50-13_9f396b488efb/events.out.tfevents.1572281413.9f396b488efb.131.5 (deflated 97%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjM_d1PFR0Cs",
        "colab_type": "text"
      },
      "source": [
        "Hyperparameter tuning using tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NNTaBYP3Z3G",
        "colab_type": "text"
      },
      "source": [
        "shifting from hard-coded values to variables for hyperparameter testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcwWr7nv46CT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size_list  = [100,1000,10000]\n",
        "lr_list = [0.01,0.001,0.0001,0.00001]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucIXWkCJ75lt",
        "colab_type": "text"
      },
      "source": [
        "testing for all the possible combinations of lr and batch_size now"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esHjRBo75Fq4",
        "colab_type": "text"
      },
      "source": [
        "rerunning using variable hyperparameters and an appropriately named summarywriter()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbVRe7Ec6Mjc",
        "colab_type": "text"
      },
      "source": [
        "as we're using different batch sizes now, going to calculate loss in a different way and account for variation as the cross entropy loss function being used here averages the loss values that are generated by a batch and returns this average<br>\n",
        "could use the reduction parameter of the cross-entropy function instead.<br>\n",
        "using the first method currently."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqO_rgHtA8wz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -r runs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdG0x2r15OsR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for batch_size in batch_size_list:\n",
        "    for lr in lr_list:\n",
        "        network = Network()\n",
        "        \n",
        "        train_loader = torch.utils.data.DataLoader(\n",
        "            train_set,batch_size=batch_size\n",
        "        )\n",
        "        optimizer = optim.Adam(network.parameters(),lr=lr)\n",
        "\n",
        "        images,labels = next(iter(train_loader))\n",
        "        grid = torchvision.utils.make_grid(images)\n",
        "\n",
        "        comment = f'batch_size = {batch_size} lr = {lr}'\n",
        "\n",
        "        tb = SummaryWriter(comment = comment)\n",
        "        tb.add_image('images',grid)\n",
        "        tb.add_graph(network,images)\n",
        "\n",
        "        for epoch in range(5):\n",
        "            total_loss = 0\n",
        "            total_correct = 0\n",
        "            for batch in train_loader:\n",
        "                images,labels = batch\n",
        "                preds = network(images)\n",
        "\n",
        "                loss = F.cross_entropy(preds,labels)\n",
        "                \n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()*batch_size\n",
        "                total_correct += get_num_correct(preds,labels)\n",
        "\n",
        "            tb.add_scalar('Loss',total_loss,epoch)\n",
        "            tb.add_scalar('Number_correct',total_correct,epoch)\n",
        "            tb.add_scalar('Accuracy',total_correct/len(train_set),epoch)\n",
        "\n",
        "            for name,param in network.named_parameters():\n",
        "                tb.add_histogram(name,param,epoch)\n",
        "                tb.add_histogram(f'{name}.grad',param.grad,epoch)\n",
        "\n",
        "            print(\n",
        "                f\"epoch: {epoch}\",\n",
        "                f\"total_correct: {total_correct}\"\n",
        "                f\"loss: {total_loss}\"\n",
        "            )\n",
        "\n",
        "        tb.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3t7NyzvHqan",
        "colab_type": "text"
      },
      "source": [
        "using cartesian products of sets instead of nested for loops for more readable code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0cBQtMsHyX3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parameters = dict(\n",
        "    lr = [0.01,0.0001],\n",
        "    batch_size = [100,1000,10000]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxritZn9IFgD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "75481daf-9f4d-4d98-f23e-c36b609d4074"
      },
      "source": [
        "param_values = [v for v in parameters.values()]\n",
        "param_values"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.01, 0.0001], [100, 1000, 10000]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npXM8W48IVi9",
        "colab_type": "text"
      },
      "source": [
        "looping over the cartesian product"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqvzmH8MIpFn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from itertools import product"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuuMSspJIdz_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "outputId": "3cd0b757-7525-4a58-982c-1dd5eb217727"
      },
      "source": [
        "for lr,batch_size in product(*param_values):\n",
        "    print(lr,batch_size)\n",
        "        "
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.01 100\n",
            "0.01 1000\n",
            "0.01 10000\n",
            "0.0001 100\n",
            "0.0001 1000\n",
            "0.0001 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iB2agLRgImaC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593
        },
        "outputId": "a4ef88a4-9dca-4154-9f5d-5e4a1eb450a6"
      },
      "source": [
        "for lr,batch_size in product(*param_values):\n",
        "    network = Network()\n",
        "    \n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_set,batch_size=batch_size\n",
        "    )\n",
        "    optimizer = optim.Adam(network.parameters(),lr=lr)\n",
        "\n",
        "    images,labels = next(iter(train_loader))\n",
        "    grid = torchvision.utils.make_grid(images)\n",
        "\n",
        "    comment = f'batch_size = {batch_size} lr = {lr}'\n",
        "\n",
        "    tb = SummaryWriter(comment = comment)\n",
        "    tb.add_image('images',grid)\n",
        "    tb.add_graph(network,images)\n",
        "\n",
        "    for epoch in range(5):\n",
        "        total_loss = 0\n",
        "        total_correct = 0\n",
        "        for batch in train_loader:\n",
        "            images,labels = batch\n",
        "            preds = network(images)\n",
        "\n",
        "            loss = F.cross_entropy(preds,labels)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()*batch_size\n",
        "            total_correct += get_num_correct(preds,labels)\n",
        "\n",
        "        tb.add_scalar('Loss',total_loss,epoch)\n",
        "        tb.add_scalar('Number_correct',total_correct,epoch)\n",
        "        tb.add_scalar('Accuracy',total_correct/len(train_set),epoch)\n",
        "\n",
        "        for name,param in network.named_parameters():\n",
        "            tb.add_histogram(name,param,epoch)\n",
        "            tb.add_histogram(f'{name}.grad',param.grad,epoch)\n",
        "\n",
        "        print(\n",
        "            f\"epoch: {epoch}\",\n",
        "            f\"total_correct: {total_correct}\"\n",
        "            f\"loss: {total_loss}\"\n",
        "        )\n",
        "\n",
        "    tb.close()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0 total_correct: 47713loss: 32399.01174902916\n",
            "epoch: 1 total_correct: 51708loss: 22450.518448650837\n",
            "epoch: 2 total_correct: 52307loss: 20951.674051582813\n",
            "epoch: 3 total_correct: 52748loss: 19802.543548494577\n",
            "epoch: 4 total_correct: 52932loss: 19280.239336192608\n",
            "epoch: 0 total_correct: 36035loss: 60779.63650226593\n",
            "epoch: 1 total_correct: 46798loss: 34281.91778063774\n",
            "epoch: 2 total_correct: 49336loss: 28776.17898583412\n",
            "epoch: 3 total_correct: 50606loss: 25469.293296337128\n",
            "epoch: 4 total_correct: 51303loss: 23690.684527158737\n",
            "epoch: 0 total_correct: 10224loss: 130767.5302028656\n",
            "epoch: 1 total_correct: 26044loss: 88107.41543769836\n",
            "epoch: 2 total_correct: 33444loss: 66302.87647247314\n",
            "epoch: 3 total_correct: 37982loss: 56926.10323429108\n",
            "epoch: 4 total_correct: 40507loss: 49828.25756072998\n",
            "epoch: 0 total_correct: 29521loss: 87715.04522562027\n",
            "epoch: 1 total_correct: 42290loss: 46644.066524505615\n",
            "epoch: 2 total_correct: 43774loss: 41928.28097939491\n",
            "epoch: 3 total_correct: 44776loss: 38981.0678422451\n",
            "epoch: 4 total_correct: 45615loss: 36993.507888913155\n",
            "epoch: 0 total_correct: 9886loss: 137413.55562210083\n",
            "epoch: 1 total_correct: 22114loss: 129861.47630214691\n",
            "epoch: 2 total_correct: 29389loss: 98660.24124622345\n",
            "epoch: 3 total_correct: 36765loss: 69750.14293193817\n",
            "epoch: 4 total_correct: 39890loss: 57638.212978839874\n",
            "epoch: 0 total_correct: 7811loss: 138373.35586547852\n",
            "epoch: 1 total_correct: 10467loss: 138266.64209365845\n",
            "epoch: 2 total_correct: 10797loss: 138161.3039970398\n",
            "epoch: 3 total_correct: 11044loss: 138044.89374160767\n",
            "epoch: 4 total_correct: 11410loss: 137905.5619239807\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15dU2IrkJApg",
        "colab_type": "text"
      },
      "source": [
        "clearing runs for the final time (done with cleaning code) and running the model and downloading zipped runs folder and viewing logged data on a local host using tensorboard."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaR2hCIhMj5C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "8fe3f209-b660-4dbe-9c51-db7cdec7b380"
      },
      "source": [
        "!zip -r runs runs/"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: runs/ (stored 0%)\n",
            "  adding: runs/Oct29_06-14-51_da21891f4764batch_size = 100 lr = 0.01/ (stored 0%)\n",
            "  adding: runs/Oct29_06-14-51_da21891f4764batch_size = 100 lr = 0.01/events.out.tfevents.1572329691.da21891f4764.123.3 (deflated 88%)\n",
            "  adding: runs/Oct29_06-17-44_da21891f4764batch_size = 10000 lr = 0.01/ (stored 0%)\n",
            "  adding: runs/Oct29_06-17-44_da21891f4764batch_size = 10000 lr = 0.01/events.out.tfevents.1572329864.da21891f4764.123.5 (deflated 12%)\n",
            "  adding: runs/Oct29_06-19-09_da21891f4764batch_size = 100 lr = 0.0001/ (stored 0%)\n",
            "  adding: runs/Oct29_06-19-09_da21891f4764batch_size = 100 lr = 0.0001/events.out.tfevents.1572329949.da21891f4764.123.6 (deflated 88%)\n",
            "  adding: runs/Oct29_06-22-07_da21891f4764batch_size = 10000 lr = 0.0001/ (stored 0%)\n",
            "  adding: runs/Oct29_06-22-07_da21891f4764batch_size = 10000 lr = 0.0001/events.out.tfevents.1572330127.da21891f4764.123.8 (deflated 12%)\n",
            "  adding: runs/Oct29_06-16-23_da21891f4764batch_size = 1000 lr = 0.01/ (stored 0%)\n",
            "  adding: runs/Oct29_06-16-23_da21891f4764batch_size = 1000 lr = 0.01/events.out.tfevents.1572329783.da21891f4764.123.4 (deflated 51%)\n",
            "  adding: runs/Oct29_06-20-39_da21891f4764batch_size = 1000 lr = 0.0001/ (stored 0%)\n",
            "  adding: runs/Oct29_06-20-39_da21891f4764batch_size = 1000 lr = 0.0001/events.out.tfevents.1572330039.da21891f4764.123.7 (deflated 49%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmTOunnhMpaR",
        "colab_type": "text"
      },
      "source": [
        "runs uploaded on github"
      ]
    }
  ]
}